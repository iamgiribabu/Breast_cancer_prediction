{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting a Categorical Feature in Machine Learning\n",
    "\n",
    "This notebook guides you through the steps to predict a categorical feature using machine learning. We will cover data preprocessing, model training, evaluation, and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Understand the Problem and Collect Data\n",
    "# For demonstration, let's use the Iris dataset from sklearn\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Preprocessing\n",
    "## Handling Missing Values\n",
    "# For simplicity, the Iris dataset does not contain missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Features\n",
    "# In the Iris dataset, the target variable is already numerical, so no encoding is needed here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[data.feature_names] = scaler.fit_transform(df[data.feature_names])\n",
    "\n",
    "# Display the scaled features\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Exploratory Data Analysis (EDA)\n",
    "## Analyze Data Distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pairplot to visualize relationships\n",
    "sns.pairplot(df, hue='target')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect and Handle Outliers\n",
    "# Box plots to visualize outliers\n",
    "for column in data.feature_names:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.boxplot(x=df[column])\n",
    "    plt.title(f'Box Plot of {column}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Feature Selection/Engineering\n",
    "# For simplicity, we'll use all features from the Iris dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split Data into Training and Test Sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Choose a Classification Algorithm\n",
    "# We'll start with a RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Train the Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators': [100, 200], 'max_depth': [10, 20, None]}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Validate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Score:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: Deployment and Monitoring\n",
    "## This step involves deploying the model to a production environment and monitoring its performance.\n",
    "## This is typically done outside the scope of a Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
